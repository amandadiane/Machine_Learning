{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (10 pts) k-Nearest Neighbors (kNN) for Credit Card Fraud Detection\n",
    "\n",
    "We will explore the application of non-parametric model to credit card fraud detection. <br/><br/>\n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud.\n",
    "\n",
    "In this question, you are required to fill in the missing code, marked as `...` and answer inline question by creating mark-down cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 pts) Loading data\n",
    "\n",
    "In this part, we will use two features only -- `V14` and `V27`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "X = ... # select 'V14', 'V27' from the df. Use .values to get numpy 2-D array\n",
    "y = ... # select the column 'Class' from df. Use .values to get a 1-D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = ... # Designate 50% of the observations as test observations. Use random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 pts) Scaling data\n",
    "\n",
    "Using StandardScaler scale features to 0 mean and unit variance Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardScaler = ... # instantiate StandardScaler\n",
    "X_train = ... # fit and tranform the training data\n",
    "X_test = ... # tranform test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 pts) Finding best k\n",
    "\n",
    "Using GridSearch find the best parameter value for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define param_map for different values of k\n",
    "param_grid = {'n_neighbors':[1, 2, 3, 4, 5, 6, 7]}\n",
    "\n",
    "# Fitting K-NN to the Training set. Use p=2 for Euclidean distance.\n",
    "kNN = ... # instantiate KNeighborsClassifier\n",
    "\n",
    "# Use 5-fold cross validation to select best k\n",
    "# use 'recall' metric to evaluate your model \n",
    "\n",
    "cmodel = ... # instantiate GridSearchCV\n",
    "cmodel.fit(X_train, y_train)\n",
    "\n",
    "best_k = ... # get the best k learned through grid search\n",
    "print('k = {} is selected using grid search'.format(best_k)\n",
    "\n",
    "# Fit a KNeighborsClassifier using the best parameters from grid search. Use p=2 for Euclidean distance\n",
    "kNN = ... \n",
    "kNN.fit(X_train, y_train)\n",
    "\n",
    "# performance on Test Data\n",
    "print('kNN recall = {}\\nkNN accuracy = {}'.format(recall_score(y_test, kNN.predict(X_test)), \n",
    "                                                   accuracy_score(y_test, kNN.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4 pts) Visualizing and comparing decison boundaries of Logistic Regression and kNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising kNN \n",
    "# on the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# copying X_test, y_test to X_set and y_set. Remeber X has two columns\n",
    "X_set, y_set = X_test, y_test \n",
    "\n",
    "# creating a meshgrid where each point is classifed using learned kNN\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, kNN.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.25, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "# You need to fill-in the code in the following few lines\n",
    "\n",
    "# scatter plot: Observations from X_set's fisrt dimension vs X_set's second dimension \n",
    "# such that observations belong to class 0\n",
    "plt.scatter(..., c = 'red', label = '0 class')\n",
    "\n",
    "# scatter plot: Observations from X_set's fisrt dimension vs X_set's second dimension \n",
    "# such that observations belong to class 1\n",
    "plt.scatter(..., c = 'green', label = '1 class')\n",
    "\n",
    "plt.title('k-NN (Test set)')\n",
    "plt.xlabel('V14')\n",
    "plt.ylabel('V27')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# We will use default parameters for Logistic Regression here\n",
    "logR = ... # instantiate LogisticRegression\n",
    "... # fit on X_train, y_train using logR\n",
    "\n",
    "# Performance on Test data\n",
    "print('Logistic Regression recall = {}\\nLogistic Regression accuracy = {}'.format(recall_score(y_test, logR.predict(X_test)), \n",
    "                                                   accuracy_score(y_test, logR.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Logistic Regression on Test data\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, logR.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.25, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "# You need to fill-in the code in the following few lines\n",
    "\n",
    "# scatter plot: Observations from X_set's fisrt dimension vs X_set's second dimension \n",
    "# such that observations belong to class 0\n",
    "plt.scatter(..., c = 'red', label = '0 class')\n",
    "\n",
    "# scatter plot: Observations from X_set's fisrt dimension vs X_set's second dimension \n",
    "# such that observations belong to class 1\n",
    "plt.scatter(..., c = 'green', label = '1 class')\n",
    "\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('V14')\n",
    "plt.ylabel('V27')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between the decision boundaries of kNN and Logistic Regression? Why would you expect the decision boundaries to be different? Explain in 2-3 sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 pts) Running time for Logistic Regression and kNN\n",
    "For run time comparison, we will use all the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-test set using full dataset\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = ... # do a train-test split and designate 50% samples as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Training time\n",
    "\n",
    "# Fitting Logistic Regression using default parameters\n",
    "logR = ... # instantiate Logistic Regression \n",
    "start_time = time.time()\n",
    "...        #fit logR on X_train, y_train\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = ... # find elapsed time\n",
    "\n",
    "print('Elapsed time for Logistic Regression training is {} seconds'.format(elapsed_time))\n",
    "\n",
    "\n",
    "# Fitting kNN using default parameters\n",
    "kNN = ... # instantiate KNeighborsClassifier\n",
    "start_time = time.time()\n",
    "...       # fir kNN on X_train, y_train\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = ... # find elapsed time\n",
    "\n",
    "print('Elapsed time for kNN classifier training is {} seconds'.format(elapsed_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice about the training time for the two classifiers? Explain why you observed this in 1-2 sentences.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Test time\n",
    "\n",
    "# Prediction using Logistic Regression\n",
    "start_time = time.time()\n",
    "y_pred = ... # use logR to predict on X_test\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = ... # find elapsed time\n",
    "\n",
    "print('Elapsed time for Logistic Regression prediction is {} seconds'.format(elapsed_time))\n",
    "\n",
    "# Prediction using kNN\n",
    "start_time = time.time()\n",
    "y_pred = ... # use kNN to predict on X_test\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = ... # find elapsed time\n",
    "\n",
    "print('Elapsed time for kNN prediction is {} seconds'.format(elapsed_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice about the test time for the two classifiers? Explain why you observed this in 1-2 sentences.**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
